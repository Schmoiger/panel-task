{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"faster r-cnn object detection.ipynb","provenance":[],"mount_file_id":"1xjzhIcDds6JgpVH-MrLrao8Ex51bqlji","authorship_tag":"ABX9TyO6BRoMPplZVF8Yh5qh4UMP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"widgets":{"application/vnd.jupyter.widget-state+json":{"8d5056a604bc47a882f4d60eecc11ae3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_b535fc2254ad4872b8ec3348b51716b8","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_07417a1ac24d4883a545851d83d568c2","IPY_MODEL_570ae78af1dc46dc925555ce0ca80941"]}},"b535fc2254ad4872b8ec3348b51716b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"07417a1ac24d4883a545851d83d568c2":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","state":{"_view_name":"ProgressView","style":"IPY_MODEL_b0dcf0d915d3439aafd961f9d373d356","_dom_classes":[],"description":"100%","_model_name":"FloatProgressModel","bar_style":"success","max":167502836,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":167502836,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19f56aa78cd9405bb3798b0a9b3c7b30"}},"570ae78af1dc46dc925555ce0ca80941":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","state":{"_view_name":"HTMLView","style":"IPY_MODEL_40bbe682199b4de2977ed90d05c21709","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"â€‹","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 160M/160M [00:43&lt;00:00, 3.81MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e964ea58002445e196054b07b71e8dbc"}},"b0dcf0d915d3439aafd961f9d373d356":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"initial","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"19f56aa78cd9405bb3798b0a9b3c7b30":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"40bbe682199b4de2977ed90d05c21709":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e964ea58002445e196054b07b71e8dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"npzRblM8IcIb"},"source":["Demonstration code for faster r-cnn object detection. With additional testing functions, to enable testing through PyTest."]},{"cell_type":"code","metadata":{"id":"LK3voS1Q6xCu"},"source":["# from https://www.learnopencv.com/faster-r-cnn-object-detection-with-pytorch/"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1_xinwWe4t8p"},"source":["# import necessary libraries\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","import torchvision.transforms as T\n","import numpy as np\n","import cv2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yOQr0O0UIwI0"},"source":["wget is not installed in Google Golab by default, so need to run shell command"]},{"cell_type":"code","metadata":{"id":"GiZMq2ox9C2u","executionInfo":{"status":"ok","timestamp":1604421186834,"user_tz":0,"elapsed":5291,"user":{"displayName":"Avi Sinharay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi41gYnHf6GREpQACKk6TFSkJG5LGvKDFJyGfgASg=s64","userId":"14737652169467905872"}},"outputId":"b58e4b5a-e2ea-4a4b-f4cc-93582a5c73df","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install wget"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Collecting wget\n","  Downloading https://files.pythonhosted.org/packages/47/6a/62e288da7bcda82b935ff0c6cfe542970f04e29c756b0e147251b2fb251f/wget-3.2.zip\n","Building wheels for collected packages: wget\n","  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for wget: filename=wget-3.2-cp36-none-any.whl size=9682 sha256=c20c955b0056bfc2c6d32f98cfe56159270530176feb2b8df17f2878eed8c311\n","  Stored in directory: /root/.cache/pip/wheels/40/15/30/7d8f7cea2902b4db79e3fea550d7d7b85ecb27ef992b618f3f\n","Successfully built wget\n","Installing collected packages: wget\n","Successfully installed wget-3.2\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"aV60pcIL6_UQ"},"source":["import wget\n","import json"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JV9GI3Cq56OI","executionInfo":{"status":"ok","timestamp":1604421199335,"user_tz":0,"elapsed":3170,"user":{"displayName":"Avi Sinharay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi41gYnHf6GREpQACKk6TFSkJG5LGvKDFJyGfgASg=s64","userId":"14737652169467905872"}},"outputId":"cd4ca34e-d75b-4df1-f9f4-0d1a96a2d40c","colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["8d5056a604bc47a882f4d60eecc11ae3","b535fc2254ad4872b8ec3348b51716b8","07417a1ac24d4883a545851d83d568c2","570ae78af1dc46dc925555ce0ca80941","b0dcf0d915d3439aafd961f9d373d356","19f56aa78cd9405bb3798b0a9b3c7b30","40bbe682199b4de2977ed90d05c21709","e964ea58002445e196054b07b71e8dbc"]}},"source":["# get the pretrained model from torchvision.models\n","# Note: pretrained=True will get the pretrained weights for the model.\n","# model.eval() to use the model for inference\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","model.eval()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Downloading: \"https://download.pytorch.org/models/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\" to /root/.cache/torch/hub/checkpoints/fasterrcnn_resnet50_fpn_coco-258fb6c6.pth\n"],"name":"stderr"},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8d5056a604bc47a882f4d60eecc11ae3","version_minor":0,"version_major":2},"text/plain":["HBox(children=(FloatProgress(value=0.0, max=167502836.0), HTML(value='')))"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["FasterRCNN(\n","  (transform): GeneralizedRCNNTransform(\n","      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n","  )\n","  (backbone): BackboneWithFPN(\n","    (body): IntermediateLayerGetter(\n","      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (bn1): FrozenBatchNorm2d(64)\n","      (relu): ReLU(inplace=True)\n","      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","      (layer1): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","            (1): FrozenBatchNorm2d(256)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(64)\n","          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(64)\n","          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(256)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer2): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(512)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(128)\n","          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(128)\n","          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(512)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer3): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(1024)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (3): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (4): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (5): Bottleneck(\n","          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(256)\n","          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(256)\n","          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(1024)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","      (layer4): Sequential(\n","        (0): Bottleneck(\n","          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048)\n","          (relu): ReLU(inplace=True)\n","          (downsample): Sequential(\n","            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","            (1): FrozenBatchNorm2d(2048)\n","          )\n","        )\n","        (1): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048)\n","          (relu): ReLU(inplace=True)\n","        )\n","        (2): Bottleneck(\n","          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn1): FrozenBatchNorm2d(512)\n","          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","          (bn2): FrozenBatchNorm2d(512)\n","          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","          (bn3): FrozenBatchNorm2d(2048)\n","          (relu): ReLU(inplace=True)\n","        )\n","      )\n","    )\n","    (fpn): FeaturePyramidNetwork(\n","      (inner_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (2): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n","        (3): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n","      )\n","      (layer_blocks): ModuleList(\n","        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      )\n","      (extra_blocks): LastLevelMaxPool()\n","    )\n","  )\n","  (rpn): RegionProposalNetwork(\n","    (anchor_generator): AnchorGenerator()\n","    (head): RPNHead(\n","      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n","      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n","    )\n","  )\n","  (roi_heads): RoIHeads(\n","    (box_roi_pool): MultiScaleRoIAlign()\n","    (box_head): TwoMLPHead(\n","      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n","      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n","    )\n","    (box_predictor): FastRCNNPredictor(\n","      (cls_score): Linear(in_features=1024, out_features=91, bias=True)\n","      (bbox_pred): Linear(in_features=1024, out_features=364, bias=True)\n","    )\n","  )\n",")"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"sDFpRIWR58U3"},"source":["# Class labels from official PyTorch documentation for the pretrained model\n","# Note that there are some N/A's\n","# for complete list check https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n","# we will use the same list for this notebook\n","COCO_INSTANCE_CATEGORY_NAMES = [\n","    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n","    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n","    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n","    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n","    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n","    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n","    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n","    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n","    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n","    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n","    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n","    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hwf4feNL5_mm"},"source":["def get_prediction(img_path, threshold):\n","    \"\"\"\n","    get_prediction\n","      parameters:\n","        - img_path - path of the input image\n","        - threshold - threshold value for prediction score\n","      method:\n","        - Image is obtained from the image path\n","        - the image is converted to image tensor using PyTorch's Transforms\n","        - image is passed through the model to get the predictions\n","        - class, box coordinates are obtained, but only prediction score > threshold\n","          are chosen.\n","\n","    \"\"\"\n","    img = Image.open(img_path)\n","    transform = T.Compose([T.ToTensor()])\n","    img = transform(img)\n","    pred = model([img])\n","    pred_class = [COCO_INSTANCE_CATEGORY_NAMES[i] for i in list(pred[0]['labels'].numpy())]\n","    pred_boxes = [[(i[0], i[1]), (i[2], i[3])] for i in list(pred[0]['boxes'].detach().numpy())]\n","    pred_score = list(pred[0]['scores'].detach().numpy())\n","    pred_t = [pred_score.index(x) for x in pred_score if x > threshold][-1]\n","    pred_boxes = pred_boxes[:pred_t+1]\n","    pred_class = pred_class[:pred_t+1]\n","    return pred_boxes, pred_class"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7BR_X-PvJD2s"},"source":["API below gives visual representation."]},{"cell_type":"code","metadata":{"id":"qhl6eLTt6E3K"},"source":["def object_detection_api(img_path, threshold=0.5, rect_th=3, text_size=3, text_th=3):\n","    \"\"\"\n","    object_detection_api\n","      parameters:\n","        - img_path - path of the input image\n","        - threshold - threshold value for prediction score\n","        - rect_th - thickness of bounding box\n","        - text_size - size of the class label text\n","        - text_th - thichness of the text\n","      method:\n","        - prediction is obtained from get_prediction method\n","        - for each prediction, bounding box is drawn and text is written\n","          with opencv\n","        - the final image is displayed\n","    \"\"\"\n","    boxes, pred_cls = get_prediction(img_path, threshold)\n","    img = cv2.imread(img_path)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","    for i in range(len(boxes)):\n","        cv2.rectangle(img, boxes[i][0], boxes[i][1], color=(0, 255, 0), thickness=rect_th)\n","        cv2.putText(img, pred_cls[i], boxes[i][0], cv2.FONT_HERSHEY_SIMPLEX,\n","                    text_size, (0, 255, 0), thickness=text_th)\n","    plt.figure(figsize=(20, 30))\n","    plt.imshow(img)\n","    plt.xticks([])\n","    plt.yticks([])\n","    plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G1W4QZFLJLI5"},"source":["Additional global parameters to locate the test data. These locations will have to be altered depending upon the environment.\n","\n","The image file has strict syntax rules:\n","- expect all images to be .jpg on a different line\n","- location to be either: 1) relative to TEST_FOLDER;\n","- or 2) full url\n","- TEST_FOLDER = ./tests/ in most environments\n","- comments have a hash at beginning of line"]},{"cell_type":"code","metadata":{"id":"DiemEjSz63pu"},"source":["TEST_FOLDER = '/content/drive/My Drive/Colab Notebooks/'\n","IMAGE_FILE = 'images.txt'\n","DETECTION_THRESHOLD = 0.8"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bcHdh3nDJ9pF"},"source":["Helper function to take contents of test data file and parse it into a Python list for processing. Includes downloading in required.\n","\n","Note: the original input text file is unaltered, and the downloaded file is not deleted after use. This means, if this function is run twice, two copies of the file will be downloaded. The OS will handle renaming the downloaded file to avoid overwriting."]},{"cell_type":"code","metadata":{"id":"4V8qYdoF7EJj"},"source":["def parse_images(test_images=TEST_FOLDER+IMAGE_FILE):\n","    # parse images list\n","    try:\n","        with open(test_images) as file:\n","            lines = [line.rstrip() for line in file]\n","        images = [line for line in lines if (line != '' and not(line.startswith('#')))]\n","    except:\n","        error_status = 'invalid image file'\n","        return error_status\n","\n","    # download into tests folder if not already downloaded\n","    for i in range(len(images)):\n","        filename = images[i].rsplit('/', 1)[-1]\n","        if images[i].startswith('http'):\n","            try:\n","                wget.download(images[i], TEST_FOLDER + filename)\n","                images[i] = TEST_FOLDER + filename\n","            except:\n","                pass\n","        else:\n","            images[i] = TEST_FOLDER + filename\n","    \n","    # if not downloadable, remove from list\n","    downloaded_images = [image for image in images if not(image.startswith('http'))]\n","    return downloaded_images"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OmhuhiElKMV3"},"source":["Helper function to run image detection list of images, then return results as a JSON file if successful."]},{"cell_type":"code","metadata":{"id":"NXKYTt337HwZ"},"source":["def detect_images(images):\n","    # run object detection on each image\n","    detections = []\n","    for image in images:\n","        _, labels = get_prediction(image, DETECTION_THRESHOLD)\n","        detections.append({\n","            'image_filename': image,\n","            'detections': labels\n","        })\n","\n","    detections_json = json.dumps(detections)\n","    return detections_json"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9m1JiC0YKlMb"},"source":["Now create the image list and detection results."]},{"cell_type":"code","metadata":{"id":"OkewMMQ07LgX","executionInfo":{"status":"ok","timestamp":1604423373655,"user_tz":0,"elapsed":1489,"user":{"displayName":"Avi Sinharay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi41gYnHf6GREpQACKk6TFSkJG5LGvKDFJyGfgASg=s64","userId":"14737652169467905872"}},"outputId":"5e1351d4-428a-46e2-d5eb-1e9468b0ef4c","colab":{"base_uri":"https://localhost:8080/"}},"source":["images = parse_images()\n","print(images)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['/content/drive/My Drive/Colab Notebooks/dog.jpg', '/content/drive/My Drive/Colab Notebooks/10best-cars-group-cropped-1542126037.jpg', '/content/drive/My Drive/Colab Notebooks/photo-1458169495136-854e4c39548a']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3TalDnYg93ip","executionInfo":{"status":"ok","timestamp":1604423458541,"user_tz":0,"elapsed":23101,"user":{"displayName":"Avi Sinharay","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gi41gYnHf6GREpQACKk6TFSkJG5LGvKDFJyGfgASg=s64","userId":"14737652169467905872"}},"outputId":"e36a178d-1c4f-4d96-d0fa-1252385b122d","colab":{"base_uri":"https://localhost:8080/"}},"source":["result_json = detect_images(images)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torchvision/ops/boxes.py:101: UserWarning: This overload of nonzero is deprecated:\n","\tnonzero()\n","Consider using one of the following signatures instead:\n","\tnonzero(*, bool as_tuple) (Triggered internally at  /pytorch/torch/csrc/utils/python_arg_parser.cpp:766.)\n","  keep = keep.nonzero().squeeze(1)\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"RpxfDExuLFhG"},"source":["Test functions to enable PyTest in build pipeline."]},{"cell_type":"code","metadata":{"id":"Cg5rrGVr7TV4"},"source":["def test_valid_json():\n","    try:\n","        r = json.loads(result_json)\n","        error = False\n","    except:\n","        error = True\n","    assert error == False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lw5bJeSf7Wrb"},"source":["def test_confirm_all_processed():\n","    if len(images) == len(json.loads(result_json)):\n","        error = False\n","    else:\n","        error = True\n","    assert error == False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mjwFYJVg7Y7o"},"source":["def test_at_least_one_result():\n","    error = False\n","    for key, value in json.loads(result_json):\n","        if value == '':\n","            error = True\n","        else:\n","            error = error or False\n","    assert error == False"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"esADRqBc8ex9"},"source":["# run tests\n","test_valid_json()\n","test_confirm_all_processed()\n","test_at_least_one_result()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3jg3Yep-LPzm"},"source":["These functions will be separated out in the final package:\n","\n","src:\n","- object_detection.py: training model, category names, get_prediction, object_detection_api;\n","\n","tests:\n","- detect_images.py: parse_images, detect_images;\n","- test_detections.py: test_valid_json, test_confirm_all_processed, test_at_least_one_result;\n","- images.txt"]}]}